## SNU FACTCHECK 데이터 중 가짜뉴스 검출 시스템에서 활용할 수 있는 데이터 크롤링 방법 도출

**<크롤링방법 1>**
 * 뉴스데이터
   * 수집 데이터 정의   
     - 가짜 : 가짜뉴스를 의도와 상관없이 허위 사실을 포함하고 있거나, 허위 사실을 말하고 있는 정치인 혹은 공직자의 발언, 찌라시, 루머 등을 포함하는 뉴스데이터   
     - 진짜 : 실제로 일어난 사건에 대한 뉴스데이터
   * 크롤링 규칙   
     - 공신력 있는 SNU FactCheck와 뉴스톱 사이트의 크롤링 날짜를 동기화   
     - 대체로 거짓, 거짓, 대체로 진실, 진실 (4종류로 분류) 중 완전히 가짜, 완전히 진실 뉴스데이터만 크롤링
   * 크롤링 고려사항    
     - 두개의 사이트에서 완전히 가짜/진실라고 판명이 되었지만, 원문 혹은 관련내용 존재 하지 않을 경우 네이버 뉴스 키워드 검색실시 하여 가장 유사하고 대표적인 뉴스데이터 수집
 * 트위터
   * 크롤링 규칙   
     - 수집된 뉴스데이터의 키워드를 추출   
     - 트위터의 고급 검색기능 활용하여 뉴스데이터 키워드 입력   
     - 관련 트윗 크롤링

**<크롤링방법 2>**
 * 뉴스데이터
   * 수집 데이터 정의 : 정치·경제적 이익을 위 해 의도적으로 언론 보도의 형식을 하고 유포된 거짓 정보
      1. 가짜뉴스의 주된 목표는 정치·경제적 이익이다. 정치적으로 상대적 이득을 얻기 위해 허위 사실을 생성·유포하여 다른 정당과 후보자를 폄하하거나 경제적으로 타인에게 손실을 주어 자신 및 이해관계자들이 이득을 얻는 것을 목적   
      2. 거짓 정보를 담고 있다. 거짓 정보는 정보 제공의 의도와 관계없이 사실과 다르게 다소 정확하지 않거나 아예 맞지 않는 정보들을 지칭하는 잘못된 정보와 정밀하게 만들어 특정한 의도로 유도하는 정보들을 지칭하는 기만적 정보를 둘 다 포함   
      3. 가짜뉴스는 언론 보도의 형식을 취한다. 형식적인 면에서 온라인 커뮤니티의 글과 같이 체계적이지 않아 자유로운 형식과 다르게 어느 정도 기사로 오인할 수 있을 만한 전문적인 언론 보도 형식을 갖추어야 한다
   * 크롤링 규칙
      1. 기간을 정하고 모든 뉴스데이터 크롤링하여 실제 , 중립 ,가짜의 3가지의 클래스로 정의
      
**<논문 1>**
 * 정리
   - SNU    : 2017년3월29일~2018년6월22일 총 679건중 완전히 가짜 19건, 완전히 진실 9건 수집
   - 뉴스탑 : 2017년4월27일~2018년6월22일 총 333건중 완전히 가짜 36건, 완전히 진실 9건 수집
   - 부족한데이터 : 2017년 1월1일 ~ 2018년6월22일 실제로 일어난 사건 진실로 분류, 명백히 가짜로 밝혀진 뉴스로 분류하여 가짜 1건, 진실 60건
   - 뉴스데이터 총 134 건 (완전히 가짜 56건, 완전히 진실 78건)
   - 메타데이터 : news_id, target(Label), title, contents, date, tweet_id, target(Label), user, date, tweets
   - 트윗데이터 : 총134건의 뉴스데이터의 키워드 추출, 고급검색 기능을 이용하여 키워드 및 날짜(2016년1월1일~2018년7월10일 한정)입력후 관련된 트윗 16,384건 수집
* 데이터 활용
  - 완전 진짜, 완전가짜 2가지의 데이터만 수집
  - 전체 뉴스데이터 토픽(주제) 모델링 수행 => 26개의 토픽으로 뉴스 데이터 구조화
  - 토픽모델링 결과와 뉴스별 주제적 특성을 입력 후 클러스터링 수행 => 30개의 이슈 그룹 도출
  - 이슈그룹 내 뉴스 기사들의 진위여부(증언의 진위 여부)에 따라 학습/검증 데이터로 분류
   - 결과 
     - 뉴스데이터 분류 : 134건중 학습데이터 77건(진실45,거짓32건), 검증데이터 57건(진실 33건,거짓24건)
     - 트윗데이터 분류 : 16,384건중 학습데이터 10,371건(진실 5,623건, 거짓4,748건) 검증데이터 6,013건(진실 3,028건, 거짓 2,985건)

**<논문 2>** 

  * 정리   
    - SNU : 2017년3월29일~2017년9월18일(6개월), 6단계로 판정된 200건의 짧은 텍스 및 메타데이터
   * 데이터 활용   
     - 1단계 : 데이터 수집과 전처리 
       - 사실과 대체로사실은 실제로 판단유보와 사실반거짓반은 중립, 대체로거짓과 거짓은 가짜로 변경   
       - 실제 , 중립 ,가짜의 3가지의 클래스 치환하고 독립변수에 해당하는 메타 데이터를 연속형 변수 및 이산형 변수로 치환   
     => Liar 메타데이터 (Subjects, Contexts/Venues, TCHC(Total Credit History Count), Home State, Current Job, Party Affiliation   
     - 2단계 : 텍스트 마이닝   
       - SNUFN 데이터셋 중에 짧은 텍스트 토픽모델링 수행 
       - 모델링 결과로 짧은 텍스트 정량화된 텍스트의 특징값들로 치환
       - 문서-토픽 가중치 행렬에서 산출되는 수치를 활용(TF-IDF 사용)   
     - 3단계 : 모델 구축
       - 짧은 텍스트 토픽모델링 이후 정량화된 텍스트의 특징값들로 치환 작업 완료   
       - SNUFN 데이터셋 학습용 vs 검증용 분류   
       - 학습된 모델 구축을 위한 학습용 데이터셋에 기계학습을 활용   
     - 4단계 : 모델 적용과 검증   
       - 실제, 중립, 가짜 중 하나로 산출되는 예측 판정이 검증용 데이터셋의 실제 판정과 일치하는 지 점검하여 예측정확도 측정   
     
* 메타데이터
  - 이산형 변수 : 주제, 언론사, 관련인, 관련인 직업, 소속정당
  - 정규화된 연속형 변수 : TCHC(Total Credit History Count)[어떠한 판정을 받았는가 : 실제,중립,가짜]
  - 토픽 모델링 : 꼬꼬마형태소 분석기(서울대학교)
  - 모델 구축 : SVM, ANN, CBR, MDA 사용

**<논문 4>** 

  * 정리   
    - 팩트체크 사이트에서 진위 여부가 확정된 이슈를 모아 이슈키워드 기반으로 트위터 데이터셋 수집
    - 수집할때, 사실 혹은 대체로 사실은 사실로 분류, 전혀 사실 아님 혹은 대체로 사실 아님은 거짓으로 분류, 절반의 사실이나 판단유보 판정된 이슈는 데이터수집대상 제외
    - 200여개의 한국어 트위터 가짜뉴스 데이터 수집 (진실, 거짓 개수 나와있지 않음)
    
**<논문 5>** 

  * 정리   
    - 언론사별 경제,사회 정치, 연예, 스포츠 카테고리를 나누어 동일 비율로 기사 수집
    - 정확히 어떠한 형식으로, 진실 vs 가짜 분류 되어있지 않음.
    - 메타데이터 형식 알수 없음.
    
**<논문 6>** 

  * 정리   
    - 가짜뉴스 1059개와 진짜 뉴스 307개는 SNU-Factcheck에서 수집
    - 부족한 진짜 뉴스는 네이버 뉴스란에서 2016년부터 2020년까지 올라온 부문별 뉴스 108730개 가운데 무작위로 752개를 수집하여 실험 데이터셋을 구성

**<논문 7>** 

  * 정리   
    - SNU Factcheck 가짜뉴스 50개 Naver News 크롤링 진짜뉴스 50개 총 100개 데이터셋 이용.
    - 메타데이터 : Date, Title, Full-text, Source, Evidence, Col.date(date of collection), Result 구성

**<논문 8>** 

  * 정리   
    - 2017년 19대 대선 때 제기된 가짜뉴스들과 관련한 주요이슈 '가짜뉴스'단어를 제목에 포함하고 있는 기사
    - 네이버 뉴스 정치카테고리 2016년3월 2017년 5월까지 '가짜뉴스'라는 단어를 제목하는 기사 수집
    - 가짜 뉴스 : 2556건 달린댓글 : 2만9,969건
    - 결론 : 가짜뉴스와 달린 댓글의 관계로 감성분석하는 작업
    
**<논문 9>** 

  * 정리   
    - 네이버 제공 블로그 검색 Open API 500개의 포스트 수집
      - 1차 : 115개의 단어로 관련된 11,459개의 포스트 수집
      - 2차 : 30개의 단어로 관련된 2,960개    
