# 가짜뉴스 탐지 기능 블록

![가짜뉴스 탐지 기능 블록3](https://user-images.githubusercontent.com/60456487/95656440-6631f400-0b49-11eb-8da1-8b8575e25fd4.png)

### 데이터 전처리
#
자연어 처리에서 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자 하는 용도에 맞게 토큰화(tokenization), 정규화(normalization) 등의 단계를 거치게 됨

1. 토큰화(tokenization)
- 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화라고 함
- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의 함
- 토큰의 기준을 단어로 하는 경우, 이를 단어 토큰화라고 부름

2. 정제(cleaning) 및 정규화(Normalization)
- 토큰화 전, 후에 데이터를 용도에 맞게 정체 및 정규화하여야 함
- 정제(cleaning) : 노이즈 데이터 제거
- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌
- 영어로 이루어진 데이터에선 정규화를 사용하여 대, 소문자를 통합하는 것도 단어의 개수를 줄여줌
- 정규표현식을 사용하여 HTML 태그나 작성 시간 등 데이터에서 공통적으로 사용되는 불필요한 글자들을 한 번에 제거할 수 있음
- 아무 의미도 갖지 않는 글자(특수 문자 등) 혹은 분석하고자 하는 목적에 맞지 않는 불필요 단어를 노이즈 데이터라고 함
- 사용 빈도가 적거나 길이가 짧은 단어가 불용어로 포함되기도 함

3. 어간 추출(Stemming)
- 형태소 분석의 단순화 버전이라고 볼 수 있음
- 정해진 규칙에 따라 단어의 어간을 자르는 작업
- 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화 시켜 단어 수를 줄이려는 방법

4. 불용어(Stopword)
- 사용 빈도 수가 높지만 의미 분석하는 데에는 크게 기여하지 않는 단어
- 불용어로 포함되는 단어들을 패키지 내에서 미리 정의하고 있거나 사용자가 직접 정의하여 불용어를 제거할 수 있음 

### Features Extraction 방법 종류
#
- DictVectorizer : 문서에서 사용된 단어의 수를 세어놓은 정보를 따로 입력받아 벡터 생성
- CountVectorizer : 문서에서 사용된 단어의 빈도 수를 세어 가중치를 설정한 벡터 생성
- TfidfVectorizer : 문서에서 사용된 단어의 빈도 수를 세어 가중치를 설정하지만 모든 문서에서 사용되는 흔한 단어에는 가중치를 주지 않음
- HashingVectorizer: 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 벡터 생성

### Classification/Classifier 종류
#
1) Fully-connected layers, 완전 연결 계층
2) Global average pooling, 평균 풀링
3) Linear support vector machines, 선형 서포트 벡터 머신
